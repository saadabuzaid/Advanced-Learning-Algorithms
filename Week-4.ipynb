{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Learning objectives**:\n",
    "* See what a decision tree looks like and how it can be used to make predictions\n",
    "* Learn how a decision tree learns from training data\n",
    "* Learn the \"impurity\" metric \"entropy\" and how it's used when building a decision tree\n",
    "* Learn how to use multiple trees, \"tree ensembles\" such as random forests and boosted trees\n",
    "* Learn when to use decision trees or wneural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a decision tree:\n",
    "A decision tree a method that makes predicts an output based on decisions set. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features\n",
    "This is how it looks like: \\\n",
    "![Decision Tree](/resources/Decision%20Tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree learning\n",
    "\n",
    "**Decision 1**: How to choose what feature to split on at each node?\n",
    "* Maxmimize purity\n",
    "\n",
    "**Decision 2**: When do you stop splitting?\n",
    "* When a node is 100% one class\n",
    "* When splitting a node will result in the tree exceeding max depth set\n",
    "* When improvements in purity score are below a threshold\n",
    "* When number of examples in a node is below a threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring impurity:\n",
    "Entropy is one way to measure the impurity of a dataset. We calculate the entropy of fraction of examples that are the class we are looking for. Entropy will be denoted to as H, while P1 is the fraction of examples that are the class we are looking for.\n",
    "\n",
    "$H$ is the entropy, defined as\n",
    "\n",
    "$$H(p_1) = -p_1 \\text{log}_2(p_1) - (1- p_1) \\text{log}_2(1- p_1)$$\n",
    "\n",
    "Remember that log here is defined to be in base 2. Run the code block below to see by yourself how the entropy. $H(p)$ behaves while $p$ varies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing a split:\n",
    "We decide what feature to split on using **Information Gain** \n",
    "Where \n",
    "\n",
    "$$\\text{Information Gain} = H(p_1^\\text{node})- \\left(w^{\\text{left}}H\\left(p_1^\\text{left}\\right) + w^{\\text{right}}H\\left(p_1^\\text{right}\\right)\\right),$$\n",
    "\n",
    "The feature with the highest infromation gain will be used as a decision node. \\\n",
    "A feature with a high infromation gain basically means that the split on this feature is useful and will help us classify classes, while a low infromation gain feature indicates that is not very useful to decide on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all-together:\n",
    "* Start with all examples at the root node\n",
    "* Calculate infromation gain for all possible features and pick the one with the highest information gain\n",
    "* Split the dataset according to selected feature\n",
    "* Keep repeating splitting process until stopping criteria is met (mentioned above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What to do if one of the features have more than 2 discrete values?\n",
    "You can use one-hot encoding of categorial features to help with this. \\\n",
    "**one-hot encoding**: If a categorial feature can take on k values, create k binary features (0 or 1 valued)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous features:\n",
    "Splitting on a continous feature (feature that is not categorial like weight) will be done using the infromation gain on each possible value, and can be selected as a decision node if it has the highest information gain \\\n",
    "Choose the 9 mid-points between the 10 examples as possible splits, and find the split that gives the highest information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression tree:\n",
    "Since we are splitting now on a regression model that predicts a number, we have no classes, we can decide what features to split on by calculating the variance of each feature and choosing the one with the highest reduction in variance. All features will have a variance of 20.51.\n",
    "The number predicted will be the average of the numbers in the training model in a certain feature leaf node."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
