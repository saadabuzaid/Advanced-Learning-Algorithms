{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Learning objectives**:\n",
    "* See what a decision tree looks like and how it can be used to make predictions\n",
    "* Learn how a decision tree learns from training data\n",
    "* Learn the \"impurity\" metric \"entropy\" and how it's used when building a decision tree\n",
    "* Learn how to use multiple trees, \"tree ensembles\" such as random forests and boosted trees\n",
    "* Learn when to use decision trees or wneural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a decision tree:\n",
    "A decision tree a method that makes predicts an output based on decisions set. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features\n",
    "This is how it looks like: \\\n",
    "![Decision Tree](/resources/Decision%20Tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree learning\n",
    "\n",
    "**Decision 1**: How to choose what feature to split on at each node?\n",
    "* Maxmimize purity\n",
    "\n",
    "**Decision 2**: When do you stop splitting?\n",
    "* When a node is 100% one class\n",
    "* When splitting a node will result in the tree exceeding max depth set\n",
    "* When improvements in purity score are below a threshold\n",
    "* When number of examples in a node is below a threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring impurity:\n",
    "Entropy is one way to measure the impurity of a dataset. We calculate the entropy of fraction of examples that are the class we are looking for. Entropy will be denoted to as H, while P1 is the fraction of examples that are the class we are looking for.\n",
    "\n",
    "$H$ is the entropy, defined as\n",
    "\n",
    "$$H(p_1) = -p_1 \\text{log}_2(p_1) - (1- p_1) \\text{log}_2(1- p_1)$$\n",
    "\n",
    "Remember that log here is defined to be in base 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing a split:\n",
    "We decide what feature to split on using **Information Gain** \n",
    "Where \n",
    "\n",
    "$$\\text{Information Gain} = H(p_1^\\text{node})- \\left(w^{\\text{left}}H\\left(p_1^\\text{left}\\right) + w^{\\text{right}}H\\left(p_1^\\text{right}\\right)\\right),$$\n",
    "\n",
    "The feature with the highest infromation gain will be used as a decision node. \\\n",
    "A feature with a high infromation gain basically means that the split on this feature is useful and will help us classify classes, while a low infromation gain feature indicates that is not very useful to decide on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all-together:\n",
    "* Start with all examples at the root node\n",
    "* Calculate infromation gain for all possible features and pick the one with the highest information gain\n",
    "* Split the dataset according to selected feature\n",
    "* Keep repeating splitting process until stopping criteria is met (mentioned above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What to do if one of the features have more than 2 discrete values?\n",
    "You can use one-hot encoding of categorial features to help with this. \\\n",
    "**one-hot encoding**: If a categorial feature can take on k values, create k binary features (0 or 1 valued)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous features:\n",
    "Splitting on a continous feature (feature that is not categorial like weight) will be done using the infromation gain on each possible value, and can be selected as a decision node if it has the highest information gain \\\n",
    "Choose the 9 mid-points between the 10 examples as possible splits, and find the split that gives the highest information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression tree:\n",
    "Since we are splitting now on a regression model that predicts a number, we have no classes, we can decide what features to split on by calculating the variance of each feature and choosing the one with the highest reduction in variance. All features will have a variance of 20.51.\n",
    "The number predicted will be the average of the numbers in the training model in a certain feature leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree ensemble\n",
    "One weakness of using a decision tree is that they are really sensitive to small changes in the data! One change can lead to changing the root node, and the rest of the decision nodes. \\\n",
    "A good way to counter this weakness is to use multiple trees to predict the output and use the most probabal output of all the trees (Tree ensemble). You can use Random forest algorithm to help create different decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process of creating Tree ensemble using random forest:\n",
    "1- Use sampling with replacement to generate different datasets for each tree; still the same number of data in the dataset, but with replacing samples. \\\n",
    "2- When building trees with a large number of features, you can use k features of all n features (selected randomly), and split on each node using highest information gain. \\\n",
    "3- Use each trees predicition to vote for the final predicition of all trees.\n",
    "\n",
    "You can also use XGBoost algorithm to help the model learn more quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision trees vs Neural network:\n",
    "**Decision Tree**:\n",
    "* Works well with structred data (tabular) e.g. housing prices, classifying animals given physical features\n",
    "* Not recommended for unstructred data. e.g. Image/voice recognition\n",
    "* The learning process for structured data is fast\n",
    "* Small decision nodes can be human interpretable\n",
    "\n",
    "**Neural network**:\n",
    "* Works well on both structured & unstructured data\n",
    "* Maybe slower than a decision tree\n",
    "* Works well with transfer learning\n",
    "* Easier to string multiple neural networks together\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
