{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Learning Algorithms course on Coursera by Andrew Ng. Week 1 notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *What is neural network*\n",
    "* Neural network or as recently re-branded deep learning was developed to try and mimic the brain neurons, so it was bioligically motived. However, it was used later to predict things by learning things and trying to predict.\n",
    "Deep learning is important too because as more information becomes available, with linear & logistic regression you reach a converging point where the performance vs amount of information is steady, and not increasing with more information provided; building deep learning networks help increase the performance vs amount of data provided most of the time.\n",
    "\n",
    "#### *How does deep learning/neural network work?*\n",
    "* In the 1950s neuroscientists thought the brain work by having multiple neurons that receives different inputs, and the neural cell calculates the output based on these inputs; the output of this neuron cell is fed to the next cell and so on. Regardless of whether this is actually how biologically the brain works, this method has proven success, and since neural network is not biologically motivated as mentioned previously, we are still using this assumption of how the brain works. <br>\n",
    "Shown below is how the neural network looks: <br>\n",
    "![Neural network hieracrchy](/resources/Neural%20network%20hierarchy.png)\n",
    "\n",
    "**Input layer/Input features**: Is the first layer where the data is passed to the neural network. Can be in the format of vectors passed to each neural cell that then learns which features to use and which ones to discard. \\\n",
    "**Hidden layer**: Is the layer(s) between the input layer and output layer. each layer is composed of a certain number of neurons that each neuron takes inputs and output a result, called activation; a layer with neurons outputs 3 activations that are to neurons downstream. This hidden layer can have multiple layers with different neuron sizes. The size of the hidden layer(s) is decided upon the network architecture (will know about this more later).\\\n",
    "**Output layer**: Is the last layer that receives all input from last hidden layer, does its calculations and outputs the final output.\n",
    "Note: Deciding what features to be used for each neuron will be time consuming specially if you are working on a big network, so what we do is we give output of each neuron from each layer as an input to every neuron in the next layer, and it is within the learning algorithm that the neuron decides what input/features to use, so you don't have to go into each neuron and decide what inputs/features to use. I think this can work will with Regularization? Since penalize features' weights and can reduce the weights to nearly 0, hence deciding what features matter? Will know about this more later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Example of a neural network application: Recoginising images*\n",
    "If we think about anything in life, it can be broken down to sub-parts, and this is how our brain makes sense of entities. Similar to that, when we use neural network to recognise faces/cars/objects, it starts going through each pixel and try to make sense of it(deep learning). In the early stages in the hidden layers, it will look for small lines and shapes, later on, it will be able form things with these small lines/shapes (nose, eyes, mouth, forehead in the case of recognising faces), and later this specific collection of eyes, mouth, forehead, etc. will correspond to an output, which will be the person's name for instance. The magical part is that the neural network learns all of this on its own without us guiding each neuron on how to learn things, it uses different training set to try to make sense things/objects by breaking it into smaller sub-parts to get by the end to the whole object. \\\n",
    "See below: \\\n",
    "![Neural network recognising faces](/resources/Neural%20network%20image%20recognition%20example.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Calculating activation vector of each layer*\n",
    "As we know, each neuron is basically a linear/logistic model that takes inputs in, learn the best weight values, and use the learnt values to predict an output; we also agreed that the an output of a layer is called activation; the output of the input layer is labeled as a[0]. Given a vector as input features, the first layer will take this input vector, and calculate output using the g($\\vec{w}$ * $\\vec{a}$ + b). This is true for each neuron in each layer, and we can find the activation for each layer using this method, and hence get the activation for the output layer, which will be the prediction of this neural network model. \\\n",
    "See below: \\\n",
    "![Activation notation](/resources/Activation%20notation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Making predictions (Inference)*\n",
    "We can use Tensor flow to create layers, activation functions for each layer, and calculate activation vector of each layer. Using the above info on how to calculate activation values through layers from input layer, index 0, to output layer, index number of layers, we build layers in tensor flow using the following syntax:\n",
    "![Creating layers using tensor flow](/resources/Building%20layers%20using%20tensor%20flow.png) \\\n",
    "**Note:** Tensor flow library deals with matrices as of object type tf.Tensor, so if you have your data in numpy, you will need to convert them to a tensor object, and back to numpy after using it in the tensor framework, if you want.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Useful tensorflow functions to remember*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.predict() # Predict the model given x_train\n",
    "tf.compile() # Learn using the X_train, y_train data given\n",
    "model = Sequential(   # Create a model with hidden layers\n",
    "    [\n",
    "        tf.keras.Input(shape=(2,)),\n",
    "        Dense(3, activation='sigmoid', name = 'layer1'),  # Layer 1 with 3 nuerons that using activation function sigmoid with name layer1\n",
    "        Dense(1, activation='sigmoid', name = 'layer2')   # Layer 2 with 1 nuerons that using activation function sigmoid with name layer2\n",
    "     ]\n",
    ")\n",
    "model.get_weights() # To get the current weights whether they are the unknown values or the ones learned if the model was compiled\n",
    "model.set_weights() # Set the weights manually"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
