{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advice for applying machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This week's learning objectives:\n",
    "* Evaluate and then modify your learning algorithm or data to improve your model's performance\n",
    "* Evaluate your learning algorithm using cross validation and test datasets.\n",
    "* Diagnose bias and variance in your learning algorithm\n",
    "* Use regularization to adjust bias and variance in your learning algorithm\n",
    "* Identify a baseline level of performance for your learning algorithm\n",
    "* Understand how bias and variance apply to neural networks\n",
    "* Learn about the iterative loop of Machine Learning Development that's used to update and improve a machine learning model\n",
    "* Learn to use error analysis to identify the types of errors that a learning algorithm is making\n",
    "* Learn how to add more training data to improve your model, including data augmentation and data synthesis\n",
    "* Use transfer learning to improve your model's performance.\n",
    "* Learn to include fairness and ethics in your machine learning model development\n",
    "* Measure precision and recall to work with skewed (imbalanced) datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can we evalulate how good a model is doing?\n",
    "Since the model is trained on a certain dataset, evaluating the model using the same dataset is not a true evaluation as the model used this dataset to learn, so it should be close to 0 loss.\n",
    "The goal is to have a model that predicts values that are not in the dataset it used to learn. \\\n",
    "If we cannot get more data to test the model with, we can split our current dataset to a training set, and testing set (maybe 70% - 30%), and use the training set to fit the model, and test how good it is using the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting dataset into training, dev set, and test set\n",
    "you might have used the entire dataset to train your models. In practice however, it is best to hold out a portion of your data to measure how well your model generalizes to new examples. This will let you know if the model has overfit to your training set.\n",
    "It is common to split your data into three parts:\n",
    "\n",
    "* ***training set*** - used to train the model\n",
    "* ***cross validation set (also called validation, development, or dev set)*** - used to evaluate the different model configurations you are choosing from. For example, you can use this to make a decision on what polynomial features to add to your dataset.\n",
    "* ***test set*** - used to give a fair estimate of your chosen model's performance against new examples. This should not be used to make decisions while you are still developing the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we need dev set?\n",
    "We can use the dev set to choose make decisions on the best architecture or polynomial degree, for instance, our model will use. We train the data using the data set, validate model deicisions using dev set; once we have decided how to build the model, we only then test the model using the test set, to make sure it is a fair evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagnosing High bias and high variance\n",
    "We can identify high bias (under fitting) and high variance(over fitting) from the cost of both the training and dev sets. \\\n",
    "A model with **high cost in training set and high cost in dev set** is a sign of **overfitting(high bias)** \\\n",
    "A model with **low cost in training set and high cost in dev set** is a sign of **undefitting(low bias)** \\\n",
    "**Regularization** also has an effect on the model's overfitting or underfitting. A very high regularization rate will lead to a high bias and under fits the model as the model is punished when increasing the weights, so most of the time f(x) will end up being approximately b, which is probably a straight line with no slope. \\\n",
    "On the other hand, a very low regularization rate, will lead to a high variance (over fitting) as the model can customize the function as it wants, as it is not punished on changing the weights values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can we establish a baseline level of performance?\n",
    "We can define whether the performance error is high or not comparted to the following:\n",
    "\n",
    "* **Human level performance**\n",
    "* **Competing algorithms performance**\n",
    "* **Guess based on experience**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias and variance in neural network:\n",
    "In a neural network, one trick that can work, not always, is if you have bias, you can increase your network size, by either increasing number of layers or number of units, and this would fix high bias, or at least it will be better than a smaller network; if you have a a high variance, you can train your model with more data and it will get better.\n",
    "However, increasing the size of your network will be a computaional cost challenge that I personally don't favor, and also getting more data to train your model can be challenging, but this is one way of getting rid of high bias or high variance. \\\n",
    "It hardly ever hurts to have a larger neural network so long as you regularize appropriately. one caveat being that having a larger neural network can slow down your algorithm. So maybe that's the one way it hurts, but it shouldn't hurt your algorithm's performance for the most part and in fact it could even help it significantly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative loop of ML development:\n",
    "\n",
    "1- **Choose architecture**: choose model, data that will be used, etc. \\\n",
    "2- **Train Model**: this is where you train the model, and most of the time you will not get best result first time so you will have to diagnose your model for better results.\\\n",
    "3- **Diagnositcs**: This is where you do all the model learning tweaking when you have high bias, variance, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding data:\n",
    "During the error analysis in the diagnostics step, you can add more data if needed per the error analysis. However, you don't always have to add new data, you can use your current data and use \\\n",
    "*data augmentation* on your current dataset, to produce a new dataset that you can use, and help your system be more robust. (e.g. slighly blur images, lower contrast, add background noises to a speech recognitiion model data, etc.). \\\n",
    "Also, you can try *generating synthetic data* that would help your model; an example would be if you are training a model that recognises letters, you can type letters in different formats and colours in a notepad, and take screenshots of the letters, and use this data to train your model too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer learning:\n",
    "You can pass the learnings of one model to another similar model (a model that is kind of similar e.g. a model that recognises digits and a model that recognises animals). \\\n",
    "Option 1: You can pass the output layers parameters as initial values, if the output classes are the same\n",
    "Option 2: You can pass the hidden layers parameters as initial values, and create a new output layer from scratch that learns on its own.\n",
    "\\\n",
    "The steps are the following: \\\n",
    "1- Supervised pretraining, which is passing the parameters to your model \\\n",
    "2- Fine tune your model to lower the cost using your learning alogrithm to lower cost\n",
    "\n",
    "The reason this can work is because within the model, similar models can have very close values, due to the similarities between the model detection training (e.g. detecting edges in image recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full cycle of a ML project:\n",
    "\n",
    "1- **Scope project**: Define project \\\n",
    "2- **Collect data**: Define and collect data \\\n",
    "3- **Train Model**: training, error analysis, iterative improvement \\\n",
    "4- **Deploy to production**: Deploy, monitor and maintain system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML model deployment:\n",
    "The ML model will be hosted on an Inference server, and the mobile app or website can then make an API call with x inputs to the inference server, and the inference server would reply with the prediction of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error metrics for skewed data:\n",
    "Skewed dataset is where a large percentage of the dataset expects 0, making the accuracy of the algorithm reasonable when it always predicts 0. \\\n",
    "We can use Precision or Recall to make sure that our algorithm is not just predicting 0 all the time with skewed data, because accuracy won't be helpful to find how good the model is with skewed data.\n",
    "\n",
    "* **Precision**: Is the number of true positives over the predicted positives\n",
    "* **Recall**: Is the number of true positives over the actual positives.\n",
    "\n",
    "These two metrics will help us identify how good our algorithm is with skewed data, and they shouldn't be 0, meaning that number of TP is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tradeoff between precision and recall:\n",
    "In the logsitics regression case, raising the threshold will lead to higher precision, as we are predicting less positives now, and will lower the recall. On the other hand, lowering the threshold lower precision and higher recall.\n",
    "\n",
    "One way to decide what algorithm to use is to use the F1 score! \\\n",
    "**F1 score** is getting the Harmonic mean of both the precision and the recall: 2PR/P+R"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
